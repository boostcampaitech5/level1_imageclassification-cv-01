{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad318b0-291d-4715-9490-df22fcdd304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "import os\n",
    "from enum import Enum\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, Subset, random_split, DataLoader, WeightedRandomSampler\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, Compose\n",
    "import torch\n",
    "import multiprocessing\n",
    "import random\n",
    "\n",
    "def cutmix(batch, alpha):\n",
    "    data, targets = batch\n",
    "\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "\n",
    "    image_h, image_w = data.shape[2:]\n",
    "    cx = np.random.uniform(0, image_w)\n",
    "    cy = np.random.uniform(0, image_h)\n",
    "    w = image_w * np.sqrt(1 - lam)\n",
    "    h = image_h * np.sqrt(1 - lam)\n",
    "    x0 = int(np.round(max(cx - w / 2, 0)))\n",
    "    x1 = int(np.round(min(cx + w / 2, image_w)))\n",
    "    y0 = int(np.round(max(cy - h / 2, 0)))\n",
    "    y1 = int(np.round(min(cy + h / 2, image_h)))\n",
    "\n",
    "    data[:, :, y0:y1, x0:x1] = shuffled_data[:, :, y0:y1, x0:x1]\n",
    "    targets = (targets, shuffled_targets, lam)\n",
    "\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "class CutMixCollator:\n",
    "    def __init__(self, a):\n",
    "        self.a = a\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch = torch.utils.data.dataloader.default_collate(batch)\n",
    "        batch = cutmix(batch, self.alpha)\n",
    "        return batch\n",
    "\n",
    "\n",
    "class CutMixCriterion:\n",
    "    def __init__(self, reduction):\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=reduction)\n",
    "\n",
    "    def __call__(self, preds, targets):\n",
    "        targets1, targets2, lam = targets\n",
    "        return lam * self.criterion(\n",
    "            preds, targets1) + (1 - lam) * self.criterion(preds, targets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e08a7-412e-4ce9-9754-c0c819321222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369481f-8031-4d88-ba5d-50f1b720f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c53d8f-6f5d-4122-9619-0c7dfaa41400",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    data_dir = '/opt/ml/input/data/train/'  \n",
    "    img_dir = f'{data_dir}images'\n",
    "    df_path = f'{data_dir}train.csv'\n",
    "    test_dir ='/opt/ml/input/data/eval/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc13bf-5f34-462c-a2e7-0943f0d8a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "means, stds =  [0.56019358, 0.52410121, 0.501457], [0.23318603, 0.24300033, 0.24567522]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc298b-9471-4ce5-9e43-9730660c67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = [\n",
    "    \".jpg\", \".JPG\", \".jpeg\", \".JPEG\", \".png\",\n",
    "    \".PNG\", \".ppm\", \".PPM\", \".bmp\", \".BMP\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf3388d-1765-49f7-831c-3fc522f65709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697158b0-1dbb-4c36-bb76-13d884552258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, GaussianBlur, RandomRotation, ColorJitter, CenterCrop, RandomHorizontalFlip\n",
    "\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "\n",
    "def get_transforms(need=('train', 'val'), img_size=[224, 224], mean =  [0.56019358, 0.52410121, 0.501457], std =[0.23318603, 0.24300033, 0.24567522] ):\n",
    "    transformations = {}\n",
    "    if 'train' in need:\n",
    "        transformations['train'] = transforms.Compose([\n",
    "            CenterCrop((384, 384)),\n",
    "            Resize((img_size[0], img_size[1])),\n",
    "            #RandomHorizontalFlip(p=0.5),\n",
    "            #RandomRotation([-5, +5]),\n",
    "            #GaussianBlur(51, (0.1, 2.0)),\n",
    "            #ColorJitter(brightness=0.5, saturation=0.5, hue=0.5),  # todo : param\n",
    "            ToTensor(),\n",
    "            Normalize(mean=mean, std=std),\n",
    "            #AddGaussianNoise(0., 1.)\n",
    "        ])\n",
    "    if 'val' in need:\n",
    "        transformations['val'] = transforms.Compose([\n",
    "            CenterCrop((384, 384)),\n",
    "            Resize((img_size[0], img_size[1])),\n",
    "            ToTensor(),\n",
    "            Normalize(mean=mean, std=std),\n",
    "        ])\n",
    "    return transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8742eb-0494-44cf-8c67-962f5ab96b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLabels(int, Enum):\n",
    "    MASK = 0\n",
    "    INCORRECT = 1\n",
    "    NORMAL = 2\n",
    "\n",
    "\n",
    "class GenderLabels(int, Enum):\n",
    "    MALE = 0\n",
    "    FEMALE = 1\n",
    "\n",
    "    @classmethod\n",
    "    def from_str(cls, value: str) -> int:\n",
    "        value = value.lower()\n",
    "        if value == \"male\":\n",
    "            return cls.MALE\n",
    "        elif value == \"female\":\n",
    "            return cls.FEMALE\n",
    "        else:\n",
    "            raise ValueError(f\"Gender value should be either 'male' or 'female', {value}\")\n",
    "\n",
    "\n",
    "class AgeLabels(int, Enum):\n",
    "    YOUNG = 0\n",
    "    MIDDLE = 1\n",
    "    OLD = 2\n",
    "\n",
    "    @classmethod\n",
    "    def from_number(cls, value: str) -> int:\n",
    "        try:\n",
    "            value = int(value)\n",
    "        except Exception:\n",
    "            raise ValueError(f\"Age value should be numeric, {value}\")\n",
    "\n",
    "        if value < 30:\n",
    "            return cls.YOUNG\n",
    "        elif value < 60:\n",
    "            return cls.MIDDLE\n",
    "        else:\n",
    "            return cls.OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8610fdb0-e059-416e-99a3-ee9011670ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskBaseDataset(Dataset):\n",
    "    num_classes = 3 * 2 * 3\n",
    "\n",
    "    _file_names = {\n",
    "        \"mask1\": MaskLabels.MASK,\n",
    "        \"mask2\": MaskLabels.MASK,\n",
    "        \"mask3\": MaskLabels.MASK,\n",
    "        \"mask4\": MaskLabels.MASK,\n",
    "        \"mask5\": MaskLabels.MASK,\n",
    "        \"incorrect_mask\": MaskLabels.INCORRECT,\n",
    "        \"normal\": MaskLabels.NORMAL\n",
    "    }\n",
    "\n",
    "    image_paths = []\n",
    "    mask_labels = []\n",
    "    gender_labels = []\n",
    "    age_labels = []\n",
    "\n",
    "    def __init__(self, data_dir, mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246), val_ratio=0.2):\n",
    "        self.data_dir = data_dir\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.val_ratio = val_ratio\n",
    "\n",
    "        self.transform = None\n",
    "        self.setup()\n",
    "        self.calc_statistics()\n",
    "\n",
    "    def setup(self):\n",
    "        profiles = os.listdir(self.data_dir)\n",
    "        for profile in profiles:\n",
    "            if profile.startswith(\".\"):  # \".\" 로 시작하는 파일은 무시합니다\n",
    "                continue\n",
    "\n",
    "            img_folder = os.path.join(self.data_dir, profile)\n",
    "            for file_name in os.listdir(img_folder):\n",
    "                _file_name, ext = os.path.splitext(file_name)\n",
    "                if _file_name not in self._file_names:  # \".\" 로 시작하는 파일 및 invalid 한 파일들은 무시합니다\n",
    "                    continue\n",
    "\n",
    "                img_path = os.path.join(self.data_dir, profile, file_name)  # (resized_data, 000004_male_Asian_54, mask1.jpg)\n",
    "                mask_label = self._file_names[_file_name]\n",
    "\n",
    "                id, gender, race, age = profile.split(\"_\")\n",
    "                gender_label = GenderLabels.from_str(gender)\n",
    "                age_label = AgeLabels.from_number(age)\n",
    "\n",
    "                self.image_paths.append(img_path)\n",
    "                self.mask_labels.append(mask_label)\n",
    "                self.gender_labels.append(gender_label)\n",
    "                self.age_labels.append(age_label)\n",
    "\n",
    "    def calc_statistics(self):\n",
    "        has_statistics = self.mean is not None and self.std is not None\n",
    "        if not has_statistics:\n",
    "            print(\"[Warning] Calculating statistics... It can take a long time depending on your CPU machine\")\n",
    "            sums = []\n",
    "            squared = []\n",
    "            for image_path in self.image_paths[:3000]:\n",
    "                image = np.array(Image.open(image_path)).astype(np.int32)\n",
    "                sums.append(image.mean(axis=(0, 1)))\n",
    "                squared.append((image ** 2).mean(axis=(0, 1)))\n",
    "\n",
    "            self.mean = np.mean(sums, axis=0) / 255\n",
    "            self.std = (np.mean(squared, axis=0) - self.mean ** 2) ** 0.5 / 255\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert self.transform is not None, \".set_tranform 메소드를 이용하여 transform 을 주입해주세요\"\n",
    "\n",
    "        image = self.read_image(index)\n",
    "        mask_label = self.get_mask_label(index)\n",
    "        gender_label = self.get_gender_label(index)\n",
    "        age_label = self.get_age_label(index)\n",
    "        multi_class_label = self.encode_multi_class(mask_label, gender_label, age_label)\n",
    "\n",
    "        image_transform = self.transform(image)\n",
    "        return image_transform, multi_class_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def get_mask_label(self, index) -> MaskLabels:\n",
    "        return self.mask_labels[index]\n",
    "\n",
    "    def get_gender_label(self, index) -> GenderLabels:\n",
    "        return self.gender_labels[index]\n",
    "\n",
    "    def get_age_label(self, index) -> AgeLabels:\n",
    "        return self.age_labels[index]\n",
    "\n",
    "    def read_image(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        return Image.open(image_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def encode_multi_class(mask_label, gender_label, age_label) -> int:\n",
    "        return mask_label * 6 + gender_label * 3 + age_label\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_multi_class(multi_class_label) -> Tuple[MaskLabels, GenderLabels, AgeLabels]:\n",
    "        mask_label = (multi_class_label // 6) % 3\n",
    "        gender_label = (multi_class_label // 3) % 2\n",
    "        age_label = multi_class_label % 3\n",
    "        return mask_label, gender_label, age_label\n",
    "\n",
    "    @staticmethod\n",
    "    def denormalize_image(image, mean, std):\n",
    "        img_cp = image.copy()\n",
    "        img_cp *= std\n",
    "        img_cp += mean\n",
    "        img_cp *= 255.0\n",
    "        img_cp = np.clip(img_cp, 0, 255).astype(np.uint8)\n",
    "        return img_cp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b0a49d-f1d8-4c03-b52c-c1f2b01c3ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = get_transforms(mean=means, std=stds)\n",
    "\n",
    "dataset = MaskBaseDataset(\n",
    "    data_dir=cfg.img_dir,\n",
    "    mean=means,\n",
    "    std=stds\n",
    ")\n",
    "\n",
    "n_val = int(len(dataset) * 0.2)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "train_set.dataset.set_transform(transform['train'])\n",
    "val_set.dataset.set_transform(transform['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e979d-a216-457c-aa28-dbc6e2f95126",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "dataset = DataLoader(\n",
    "    dataset,\n",
    "    num_workers=multiprocessing.cpu_count() // 2,\n",
    "    shuffle=True,\n",
    "    pin_memory=use_cuda,\n",
    "    drop_last=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1197ca7-b854-4ea4-8c69-b4f88ead5adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate = CutMixCollator(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f2f75b-d650-49fb-98ef-51f09f9136ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=[]\n",
    "for images, labels in train_set:\n",
    "    labels = int(labels)\n",
    "    y_train.append(labels)\n",
    "class_sample_count = [2745, 2050, 415, 3660, 4085, 545, 549, 410, 83, 732, 817, 109, 549, 410, 83, 732, 817, 109] \n",
    "weight = 1. / np.array(class_sample_count)\n",
    "samples_weight = np.array([weight[t] for t in y_train])\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4323c946-a7ed-42a2-93c4-94d1ff5013ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=64,\n",
    "    num_workers=multiprocessing.cpu_count() // 2,\n",
    "    shuffle=True,\n",
    "    #sampler= sampler,\n",
    "    collate_fn = collate,\n",
    "    pin_memory=use_cuda,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=64,\n",
    "    num_workers=multiprocessing.cpu_count() // 2,\n",
    "    shuffle=False,\n",
    "    pin_memory=use_cuda,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942f226-23c4-4306-a1f5-c6235860a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet_MultiLabel(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=18):\n",
    "        super(EfficientNet_MultiLabel, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.network = EfficientNet.from_pretrained('efficientnet-b4', in_channels=self.in_channels, num_classes=self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.network(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f1419-f0d7-4228-a164-ad13a44b49bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNet_MultiLabel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeecaca-2b9d-4846-8a3b-41c3e195fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None,\n",
    "                 gamma=2., reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4fac56-de8a-4901-91f3-1d24b115b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac159bf-ea37-40a5-bc73-b065886f3671",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3129da70-3d5b-4a9f-b040-a726c9371b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, num):\n",
    "        self.val = val\n",
    "        self.sum += val * num\n",
    "        self.count += num\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b525537-b5f6-41b5-a319-c5d75bb6cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project='eff_cut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4defb82-003f-4119-87a5-93a1f166ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "EPOCH = 15\n",
    "criterion_1 = FocalLoss()\n",
    "#criterion_2 = F1Loss() #CrossEntropyLossWithClassBalancing(num_classes, class_weights) #torch.nn.CrossEntropyLoss # default: cross_entropy\n",
    "criterion_3 = CutMixCriterion(reduction='mean')\n",
    "criterion_4 = nn.CrossEntropyLoss()\n",
    "opt_Adam = torch.optim.Adam(model.parameters(), lr=LR, betas=(0.9,0.99))\n",
    "#scheduler = StepLR(optimizer, args.lr_decay_step, gamma=0.5)\n",
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "X=[]\n",
    "y=[]\n",
    "#-- Training \n",
    "for epoch in range(EPOCH):\n",
    "    print(epoch)\n",
    "    # train loop\n",
    "    model.train()\n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    \n",
    "    loss_meter = AverageMeter()\n",
    "    accuracy_meter = AverageMeter()\n",
    "    \n",
    "    for idx, train_batch in enumerate(train_loader):\n",
    "        inputs, labels = train_batch\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        if isinstance(labels, (tuple, list)):\n",
    "            targets1, targets2, lam = labels\n",
    "            labels = (targets1.to(device), targets2.to(device), lam)\n",
    "        \n",
    "        opt_Adam.zero_grad()\n",
    "\n",
    "        outs = model(inputs)\n",
    "    \n",
    "        loss = criterion_3(outs,labels)#0.4*criterion_1(outs, labels)+0.6*criterion_2(outs, labels) #criterion()(outs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        opt_Adam.step()\n",
    "\n",
    "        _, preds = torch.max(outs, dim=1)\n",
    "\n",
    "        loss_ = loss.item()\n",
    "\n",
    "        num = inputs.size(0)\n",
    "\n",
    "        if isinstance(labels, (tuple, list)):\n",
    "            targets1, targets2, lam = labels\n",
    "            correct1 = preds.eq(targets1).sum().item()\n",
    "            correct2 = preds.eq(targets2).sum().item()\n",
    "            accuracy = (lam * correct1 + (1 - lam) * correct2) / num\n",
    "        else:\n",
    "            correct_ = preds.eq(targets).sum().item()\n",
    "            accuracy = correct_ / num\n",
    "            \n",
    "        loss_meter.update(loss_, num)\n",
    "        accuracy_meter.update(accuracy, num)\n",
    "        #print(loss_meter.avg)\n",
    "        print(f'Train/Loss: {loss_meter.avg:.2f} ||', f'Train/Acc: {accuracy_meter.avg:.2%}')\n",
    "        wandb.log({\"Train/loss\": loss_meter.avg,\"Train/acc\": accuracy_meter.avg})\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss_items = AverageMeter()\n",
    "        val_acc_items = AverageMeter()\n",
    "        figure = None\n",
    "        \n",
    "        for val_batch in val_loader:\n",
    "            inputs, labels = val_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outs = model(inputs)\n",
    "\n",
    "            loss_item = criterion_1(outs,labels) #(0.4*criterion_1(outs, labels)+0.6*criterion_2(outs, labels)).item() #criterion()(outs, labels).item()\n",
    "\n",
    "            _, preds = torch.max(outs, dim=1)\n",
    "\n",
    "            loss_ = loss_item.item()\n",
    "            correct_ = preds.eq(labels).sum().item()\n",
    "            num = inputs.size(0)\n",
    "\n",
    "            val_loss_items.update(loss_, num)\n",
    "            val_acc_items.update(correct_, 1)\n",
    "\n",
    "        accuracy = val_acc_items.sum / len(val_loader.dataset)\n",
    "        print(f'Val/Loss: {val_loss_items.avg:.2f} ||', f'Val/Acc: {accuracy:.2%}')\n",
    "        wandb.log({\"Val/loss\": val_loss_items.avg,\"Val/acc\": accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f7afa6-afab-45fa-b56f-1bedb306a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb94ef-a579-4268-bffe-75cd44a77bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(os.path.join(cfg.test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(cfg.test_dir, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f978b-f281-4cc7-9f81-d06337b8bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    CenterCrop((384, 384)),\n",
    "    Resize((224, 224), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=means, std=stds),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c79cc3-d445-4f1b-acf4-373755014432",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "effi_ensemble = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        effi_ensemble.extend(pred.cpu().numpy())\n",
    "        \n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(cfg.test_dir, 'effi_cut.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c895ea9-5255-4015-94e0-647bb3708334",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_preds = pd.read_csv('vit_preds.csv',index_col=0)\n",
    "vit_preds = vit_preds.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13f230e-c171-4742-8747-2ef781fdb98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "effi_ensemble = pd.DataFrame(effi_ensemble,columns=range(18))\n",
    "#effi_ensemble.to_csv('effi_cut_ensemble.csv')\n",
    "effi_ensemble = effi_ensemble.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9296c86e-dbc0-4a9a-bbc5-4b204f216d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_prediction = []\n",
    "pred = 0.4*(vit_preds) + 0.6*effi_ensemble\n",
    "pred = np.argmax(pred,axis=1)\n",
    "pred\n",
    "submission['ans'] = pred\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(cfg.test_dir, 'vit_effi_cut.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92785e92-0e02-42a1-a0d0-a454d0fc7581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
